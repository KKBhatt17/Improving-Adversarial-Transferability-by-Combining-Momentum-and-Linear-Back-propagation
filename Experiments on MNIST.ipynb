{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiments on MNIST.ipynb","provenance":[],"collapsed_sections":["dbwMWW1Ce7Dq","Pd3dLjycfoQY"],"authorship_tag":"ABX9TyMSvMDuR9jY5T71neRIjdD/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"NTqz6AEmkasJ","executionInfo":{"status":"ok","timestamp":1649315335386,"user_tz":-330,"elapsed":443,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","\n","import numpy as np\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","import os\n","import datetime"]},{"cell_type":"code","source":["class NN_MNIST(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(1, 8, (3,3), padding='same'),     # 28 x 28 x 8\n","            nn.MaxPool2d(kernel_size=(2,2), stride=2),   # 14 x 14 x 8\n","            nn.ReLU(),\n","            nn.Conv2d(8, 32, (3,3), padding='same'),     # 14 x 14 x 32\n","            nn.MaxPool2d(kernel_size=(2,2), stride=2),   #  7 x  7 x 32\n","            nn.ReLU(),\n","            nn.Flatten(),                               # 1568\n","            nn.Linear(1568, 200),                       # 200\n","            nn.Linear(200, 10),                         # 10 \n","            # nn.Softmax(dim=1)                           # 10\n","        )            \n","\n","    def forward(self, x):\n","        return nn.Softmax(dim=1)(self.model(x))\n","\n","    def zero_grad(self):\n","        return self.model.zero_grad()\n","    \n","    def train(self, dataset, args):\n","        train_dataset, test_dataset = dataset\n","        epochs = args['base_epochs']\n","        lr = args['base_lr']\n","        batch_size = args['base_batch_size']\n","        weight_decay = args['base_weight_decay']\n","        save_freq = args['save_freq']\n","\n","        save_dir = os.path.join(args['save_dir'], 'MNIST', str(datetime.datetime.now())[:-10])\n","        os.makedirs(save_dir)\n","\n","        loss_list = []\n","        test_loss_list = []\n","        batch_loss_list = []\n","\n","        train_dl = DataLoader(train_dataset, batch_size=batch_size)\n","        test_dl = DataLoader(test_dataset, batch_size=batch_size)\n","        loss_fn = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","        for i in range(epochs):\n","            epoch_loss = 0\n","            total = 0\n","            correct = 0\n","            for idx, data in enumerate(train_dl):\n","                inputs = data[0]\n","                targets = data[1]\n","                outputs = self.model(inputs)\n","                \n","                _, predicted = torch.max(outputs, 1)\n","                total += targets.size(0)\n","                correct += (predicted == targets).sum().item()\n","                \n","                loss = loss_fn(outputs, targets)\n","                batch_loss_list.append(loss)\n","                epoch_loss += loss.item()\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","            loss_list.append(epoch_loss)\n","\n","            with torch.no_grad():\n","                test_total = 0\n","                test_correct = 0\n","                test_epoch_loss = 0\n","                for idx, data in enumerate(test_dl):\n","                    inputs = data[0]\n","                    targets = data[1]\n","                    outputs = self.model(inputs)\n","                    \n","                    _, predicted = torch.max(outputs, 1)\n","                    test_total += targets.size(0)\n","                    test_correct += (predicted == targets).sum().item()\n","\n","                    loss = loss_fn(outputs, targets)\n","                    test_epoch_loss += loss.item()\n","\n","            test_loss_list.append(test_epoch_loss)\n","            print(f'Epoch: {i+1}/{epochs}  |  Training Accuracy: {round(correct/total*100,2)},   Validation Accuracy: {round(test_correct/test_total*100,2)}')\n","\n","            if (i+1)%save_freq == 0:\n","                model_path = os.path.join(save_dir, f'model_{i+1}')\n","                torch.save(self.model.state_dict(), model_path)\n","                print('Saved model at: ', model_path)\n","\n","        print('Finished Training')\n","        \n","        plt.figure()\n","        plt.plot(batch_loss_list)\n","        plt.xlabel('Iterations')\n","        plt.ylabel('Loss')\n","        plt.savefig(os.path.join(save_dir, 'loss_vs_iters.png'))\n","        \n","        plt.figure()\n","        plt.plot(loss_list, label='train')\n","        plt.plot(test_loss_list, label='val')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.savefig(os.path.join(save_dir, 'loss_vs_epochs.png'))\n"],"metadata":{"id":"KCVa2zNhmNO3","executionInfo":{"status":"ok","timestamp":1649315335947,"user_tz":-330,"elapsed":4,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class NN_MNIST_linear(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model1 = nn.Sequential(\n","            nn.Flatten(),                               # 784\n","            nn.Linear(784, 200)                         # 200\n","        )\n","        self.model2 = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Linear(200, 10),                         # 10 \n","            nn.Softmax()   \n","        )\n","        self.loss_fn = nn.CrossEntropyLoss()\n","    \n","    def forward1(self, inputs):\n","        return self.model1(inputs)\n","    \n","    def forward2(self, inputs):\n","        return self.model2(inputs)\n","\n","    def linbp_grad(self, x, y):\n","        output = self.forward1(x)\n","        output.backward(retain_graph=True)\n","        output = self.forward2(output)\n","        loss = self.loss_fn(output, y)\n","        loss.backward()\n"],"metadata":{"id":"R9hfqcFFgJeG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = NN_MNIST()\n","model_linear = NN_MNIST_linear()\n","# x = torch.rand(1,28,28)\n","x = -1*torch.ones(1, 28, 28)\n","model_linear(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzPJ-_bahhdn","executionInfo":{"status":"ok","timestamp":1649177046763,"user_tz":-330,"elapsed":7,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}},"outputId":"6944a303-e897-4759-9e6f-eeea3cc87073"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1064, 0.0928, 0.0928, 0.1185, 0.0928, 0.0970, 0.0928, 0.1015, 0.1009,\n","         0.1047]], grad_fn=<SoftmaxBackward0>)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# len(list(nn_linear))\n","len([layer for layer in model_linear.modules() if not isinstance(layer, nn.Sequential) and not isinstance(layer, NN_MNIST_linear)])\n","# nn_linear.modules()\n","\n","# for layer in model.modules():\n","#     if not isinstance(layer, nn.Sequential) and not isinstance(layer, NN_MNIST):\n","#         print(layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zxw3bzUIk6mC","executionInfo":{"status":"ok","timestamp":1649177393435,"user_tz":-330,"elapsed":516,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}},"outputId":"6bbbc762-66bb-48e2-c267-75fbd089754d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["def linbp_grad(x, y):\n","    # layer_list = [module for module in model_linear.modules() if not isinstance(module, nn.Sequential) and not isinstance(module, NN_MNIST_linear)]\n","    \n","    "],"metadata":{"id":"LOfz_czVkVmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def init_weights(m):\n","#     if isinstance(m, nn.Linear):\n","#         torch.nn.init.uniform_(m.weight, a=0, b=1)\n","#         m.bias.data.fill_(0.01)\n","\n","# nn_linear.apply(init_weights)\n","# nn_linear(x)"],"metadata":{"id":"2cuBQhXjjW-n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Linear Backpropagation on Toy examples"],"metadata":{"id":"dbwMWW1Ce7Dq"}},{"cell_type":"markdown","source":["### Method 1\n","use backward() just before the activation and then explicitly multiply the gradient of loss wrt the logits"],"metadata":{"id":"Pd3dLjycfoQY"}},{"cell_type":"code","source":["x = torch.tensor([-2.0], requires_grad=True)\n","W = torch.tensor([3.0], requires_grad=True)\n","output = W*x\n","# output.backward(retain_graph=True)\n","output = F.relu(output)\n","# output.retain_grad()\n","loss = output*torch.tensor([2.0])  ## say\n","loss.backward()\n","# output = linbp_relu(x)\n","print('output: ', output)\n","with torch.no_grad():\n","    print('grad x:', x.grad)\n","    # print('final grad: ', x.grad*output.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nA0W-QQ98UpR","executionInfo":{"status":"ok","timestamp":1649174220975,"user_tz":-330,"elapsed":411,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}},"outputId":"220e295a-c7bb-4d69-d5db-80a9dd792fe8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output:  tensor([0.], grad_fn=<ReluBackward0>)\n","grad x: tensor([0.])\n"]}]},{"cell_type":"code","source":["x = torch.tensor([-2.0], requires_grad=True)\n","W = torch.tensor([3.0], requires_grad=True)\n","output = W*x\n","output.backward(retain_graph=True)\n","output = F.relu(output)\n","output.retain_grad()\n","loss = output*torch.tensor([2.0])  ## say\n","loss.backward()\n","# output = linbp_relu(x)\n","print('output: ', output)\n","with torch.no_grad():\n","    print('grad x:', x.grad)\n","    print('final grad: ', x.grad*output.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUS_uNcL-9jN","executionInfo":{"status":"ok","timestamp":1649174085032,"user_tz":-330,"elapsed":368,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}},"outputId":"543c6042-e1cf-4609-f1e3-73d05f5d721c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output:  tensor([0.], grad_fn=<ReluBackward0>)\n","grad x: tensor([3.])\n","final grad:  tensor([6.])\n"]}]},{"cell_type":"code","source":["def linbp_relu(x):\n","    x_p = F.relu(-x)\n","    x = x + x_p.data\n","    return x"],"metadata":{"id":"OSgHNoZd7OBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"eXF4umxiZ7bo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Trials"],"metadata":{"id":"Ag5cePap2daW"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"pSGPqKe52fpM","executionInfo":{"status":"ok","timestamp":1649315322106,"user_tz":-330,"elapsed":6006,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["class Normalize(nn.Module):\n","    def __init__(self,):\n","        super(Normalize, self).__init__()\n","        self.ms = [(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)]\n","    def forward(self, input):\n","        x = input.clone()\n","        for i in range(x.shape[1]):\n","            x[:,i] = (x[:,i] - self.ms[0][i]) / self.ms[1][i]\n","        return x"],"metadata":{"id":"lbgrJjlL2vNP","executionInfo":{"status":"ok","timestamp":1649315360554,"user_tz":-330,"elapsed":9,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["model = NN_MNIST()\n","\n","model = nn.Sequential(\n","    Normalize(),\n","    model\n",")"],"metadata":{"id":"GZLoKhAM2lx1","executionInfo":{"status":"ok","timestamp":1649315386482,"user_tz":-330,"elapsed":7,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wk4H0rLF242M","executionInfo":{"status":"ok","timestamp":1649315392801,"user_tz":-330,"elapsed":517,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}},"outputId":"9c203a5d-59a8-4715-d9af-75a66a5f1017"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Normalize()"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["model[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BPwL67BV26ZE","executionInfo":{"status":"ok","timestamp":1649315399262,"user_tz":-330,"elapsed":10,"user":{"displayName":"Hitvarth Diwanji","userId":"14343999434773841196"}},"outputId":"562fbba3-b12b-4575-f702-203823e6269e"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NN_MNIST(\n","  (model): Sequential(\n","    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n","    (1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (2): ReLU()\n","    (3): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n","    (4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): ReLU()\n","    (6): Flatten(start_dim=1, end_dim=-1)\n","    (7): Linear(in_features=1568, out_features=200, bias=True)\n","    (8): Linear(in_features=200, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":[""],"metadata":{"id":"pTmG3Wlh28CC"},"execution_count":null,"outputs":[]}]}